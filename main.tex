\documentclass[12pt]{article}
\usepackage[spanish, es-tabla]{babel} % Para que las tablas digan "tabla" en vez de "cuadro"
\usepackage{afterpage}
\usepackage{graphicx}
\usepackage[pdftex,bookmarks,colorlinks,breaklinks]{hyperref}  
\usepackage[utf8]{inputenc}
\usepackage{epstopdf}
\usepackage{fullpage}
\usepackage{epigraph}
\usepackage{alltt}
\usepackage{url}
\usepackage{colortbl}
\usepackage{lscape}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{rotating} % Imágenes de lado
\usepackage{array} % Para las tablas
\usepackage{makecell} % lo mismo
\usepackage{parskip} % Paquete para manejar espaciado entre párrafos
\setlength{\parindent}{15pt} % Sangría al inicio de párrafos
\usepackage[font=small,labelfont=bf]{caption} % Hacer que los caption (texto debajo de imagenes o cosas, sea mas chico)
\hypersetup{citecolor=red, linkcolor=blue} % Color de los links y las citaciones
\graphicspath{ {./figures/} } % Agrupar las imágenes en una carpeta
\linespread{1.3} % Espaciado
\renewcommand\theadfont{\bfseries} % Para que el título de las tablas sea en negrita

\begin{document}

% Logo
\begin{figure}[!ht]
    \vspace{-5mm}
    \centering
        \includegraphics[scale=0.14]{logo.png}
\end{figure}

\begin{center}
\begin{LARGE}
    \rule{14cm}{0.5mm}
    \textbf{Implementación de un pipeline para el análisis de bacterias} \\
    \vspace{1mm} % vspace
    \rule{14cm}{0.5mm}
    \vspace{1cm}
\end{LARGE}
\end{center}
\begin{center}
    \begin{large}
        Memoria para optar al título de ingeniero civil en bioinformática.
    \end{large}
\end{center}
\vfill % vfill es para rellenar espacio vertical, o sea, patea todo hacia abajo
\thispagestyle{empty} % para que no se añada el número de página
\noindent % Para no tener sangría al inicio de un texto
\emph{\textbf{Nombre:}} \hfill \emph{\textbf{Fecha:}} \\ % hfill rellena el espacio horizontal, osea, patea todo a la derecha
Benjamín Astudillo Alarcón \hfill Julio, 2024\\
\emph{\textbf{Profesor Tutor:}} \hfill \emph{\textbf{Profesor Informante:}} \\
Dra. Karen Oróstica \hfill  Dr. José Reyes\\
\emph{\textbf{Profesor Encargado:}}\\
Dra. Wendy Gonzalez \\
\newpage
\emph{Esta página es dejada en blanco a propósito}
%\newpage
%\section*{Agradecimientos}

%\hfill \emph{B. Astudillo}

\newpage
{
    \hypersetup{linkcolor=black} % color de las secciones en la página
    \tableofcontents % Genera la tabla de contenidos (Indice)
}
\newpage
{
    \hypersetup{linkcolor=black} % color de las secciones en la página
    \listoftables % Genera la tabla de contenidos (Indice)
}
\newpage
{
    \hypersetup{linkcolor=black} % color de las secciones en la página
    \listoffigures % Genera la tabla de contenidos (Indice)
}
\newpage
\section*{Resumen}
La bioinformática es un campo de la ciencia en el cual confluyen varias disciplinas: biología, computación y tecnología de la información. Esta definición procede del NCBI (Centro Nacional para la Información Biotecnológica de EUA) y tiene como objetivo crear bases de datos públicas de libre acceso, crear investigación en biología computacional, desarrollar programas para análisis de secuencias y difundir la información biomédica.\par 
En la actualidad se disponen de varias bases de datos con información biológica; los investigadores pueden acceder a los datos existentes y suministrar o revisar datos, así como utilizar la información para realizar análisis comparativos entre secuencias de nucleótidos y aminoácidos. En el campo de la resistencia bacteriana, la bioinformática se emplea para la asignación funcional de genes por medio de comparaciones con secuencias (ADN o proteínas) previamente existentes en el GenBank. En este mismo sentido y debido al aumento de enfermedades provocadas por agentes patógenos infecciosos, se hace imprescindible continuar investigando las bacterias para identificar características que puedan mejorar  la resistencia o la calidad de los antibióticos, así como las terapias génicas.\par
En razón de lo anterior, es que este trabajo tiene como propósito desarrollar un Pipeline Bioinformático que integre herramientas  computacionales necesarias para el procesamiento y  análisis genómico de bacterias, que establezca un flujo predefinido que permita automatizar el proceso, haciendo que cada etapa del trabajo sea visible, facilitando de esta manera, el control sobre el avance de la tarea, sin necesidad de un aumento de los recursos,  disminuyendo además, la posibilidad de errores y  propiciando una evolución constante, que permita mayor y mejor accesibilidad a otros investigadores.\par
Para este efecto, se priorizaron métodos de análisis genómicos y se automatizó un reporte final, además de estructurar la implementación y empaquetamiento del pipeline.\par
Se pretende que el pipeline sea utilizado por investigadores y profesionales no especializados en bioinformática, mejorando así la accesibilidad y reproducibilidad  de los análisis genómicos. Así también, permitirá integrar el pipeline con otras herramientas y base de datos relevantes, facilitando la interoperabilidad y la reutilización en proyectos futuros.\par

\vspace{10pt}
\textbf{\emph{Palabras claves: pipeline, anotación, bioinformática, secuenciación, resistencia bacteriana, datos genómicos.}}



\newpage
\section{Introducción}

Se considera genoma a toda la información biológica que tiene un organismo. 
La mayoría de los genomas, incluyendo el ser humano y toda forma de vida celular, 
está compuesto por la totalidad de ADN [1], que se encuentra en una célula, 
ya sea en los cromosomas o en organelos llamados mitocondrias. Por ejemplo, 
si se habla del genoma humano, este se refiere a la totalidad de ADN presente en una 
célula reproductiva normal. Así pues, en los últimos años, se ha perfeccionado la 
tecnología para obtener la secuencia completa de ADN de una especie de una forma rápida 
y eficiente. Por tanto, actualmente es posible saber el genoma de muchos organismos 
vivos [1].

Así mismo y a través de técnicas de análisis genómico, se logra caracterizar 
bacterias, permitiendo entender cómo funcionan y qué papel desempeñan en diferentes 
contextos biológicos. Igualmente, los diferentes tipos de análisis genómicos varían 
dependiendo del objetivo de cada investigación. Sin embargo, para poder comenzar 
cualquier tipo de caracterización, es crucial disponer de datos de secuenciación del
genoma completo (\emph{WGS}). El ensamblaje del genoma, a partir de estos datos, es un paso 
fundamental en este proceso, ya que proporciona una representación coherente y completa 
del genoma bacteriano. Este ensamblaje permite realizar análisis más precisos y 
detallados, como la anotación de genes, la identificación de regiones funcionales, la 
comparación con otros genomas y detección de ARG [2].

En definitiva, el ensamblaje bacteriano, implica un preprocesamiento de las 
lecturas, un montaje de las lecturas y en varios casos un paso adicional de pulido el 
montaje. En este sentido, lograr desarrollar un ensamblaje de calidad, significa que 
se deben generar contigs (fragmentos continuos de secuencia) con pocos espacios restantes, 
baja tasa de error, entre otros. Es decir, la generación de un buen rendimiento depende 
totalmente de las herramientas y parámetros utilizados en las plataformas de 
secuenciación [2]. 

En resumen, una vez que se tiene el montaje realizado, se pueden hacer los análisis 
respectivos, ya sea para la identificación de las funciones biológicas, procesos 
celulares, o ontología genética entre otros [3]. Para realizar los análisis es necesario 
la utilización de diversas herramientas bioinformáticas o recurrir a flujos de trabajo 
computacionales, con herramientas y recursos específicos que ayuden o 
faciliten el proceso [2].

En suma, será cada vez más común acceder a grandes bancos de información genómica, 
utilizar algoritmos sofisticados y realizar análisis de datos desde nuestras 
terminales personales, sin tener que generar nuestra propia infraestructura informática o 
contar con el apoyo de expertos en informática. Es así como, la investigación biológica que 
utiliza información genómica se convierte cada vez más en una actividad computacional, 
analizando cantidades gigantescas de datos desde la comodidad de nuestras computadoras 
personales [4].

\subsection*{Desafíos del Análisis de Big Data en Genómica Bacteriana}

Cuando hablamos de Big Data nos referimos a conjuntos de datos o combinaciones de 
conjuntos de datos cuyo tamaño (volumen), complejidad (variabilidad) y velocidad de 
crecimiento dificultan su gestión, procesamiento o análisis mediante tecnologías y 
herramientas convencionales, tales como bases de datos relacionales y estadísticas 
convencionales o paquetes de visualización, dentro del tiempo necesario para que 
sean útiles [5].

Por otra parte, la genómica se encarga de determinar y analizar la secuencia 
completa de ADN de un organismo, es decir, de un genoma. Por lo tanto, es el 
estudio de la totalidad de los genes de un organismo para comprender su organización, 
función, interacción y evolución molecular [1]. En cuanto a los desafíos que plantea 
el análisis de Big Data en la genómica bacteriana, podemos señalar que el avance de la 
secuenciación de alto rendimiento o nueva generación [6], permite obtener una rebaja en 
los costos de rendimiento y un gran aumento en el tamaño de los datos de secuenciación 
generados, permitiendo una mejor claridad de la taxonomía y una mejor capacidad para 
evaluar las diferentes capacidades del sistema secuenciado [7].

Todo lo anterior, significa que cada vez tenemos una cantidad mayor de datos, 
lo que nos lleva a utilizar técnicas y tecnologías de big data, data mining y 
data science para procesarlos y extraer información útil de ellos [8].

\subsection*{Herramientas y experiencias de desarrollo en tareas de procesamiento y gestión de datos en bioinformática}

El aumento en el poder de procesamiento y sofisticación de las herramientas y técnicas 
analíticas ha dado como resultado para la gestión y procesamiento de datos, la 
creación de estructuras y programas que proporcionan almacenamiento, funcionalidad y 
receptividad a las consultas, que van más allá de las bases de datos destinadas a 
transacciones. El área de bioinformática, en este sentido, enfoca parte de su trabajo 
en el procesamiento de datos, donde se requieren herramientas con suficiente flexibilidad 
con el fin de dar soporte a las tareas propias del manejo de formatos de almacenamiento y 
métodos de procesamiento, reduciendo de esta manera, la dimensionalidad del espacio de 
trabajo [9].

\subsection*{Comandos Unix}

Los comandos Unix corresponden a  pequeños programas que se pueden ejecutar de varias 
formas, entre ellas, de forma interactiva desde una terminal. Gran parte de los servidores 
están basados en sistemas Unix, como Solaris, BSD, HP-UX, así como las distintas 
distribuciones de Linux que también son muy utilizadas en el ámbito personal, entre 
ellas Ubuntu, Debian, CentOS, Red Hat, etc. También MacOS está basado en Unix [10].

Estos programas se pueden encontrar  al introducir un comando en una terminal, el 
intérprete de comandos lo busca en directorios incluidos en lo que se denomina PATH. El PATH 
es una variable de entorno que contiene una lista con los directorios del sistema 
más comunes para almacenar ejecutables, por ejemplo /bin o /usr/bin. Para ver los 
directorios que contiene el PATH, podemos ejecutar el comando echo \$PATH. Si se 
ejecuta un comando que el sistema no encuentra, es debido a que no está en ninguno de 
estos directorios [10].

En sistemas de tipo Unix, es bastante común usar la terminal para moverse de un 
lado a otro y realizar las acciones desde allí, en lugar de usar un explorador de 
archivos o cualquier otro programa con interfaz gráfica [10]. 

\subsection*{Bash Scripting}

Un Bash Shell Script corresponde a un archivo de texto sin formato que 
contiene un conjunto de varios comandos que normalmente escribimos en la 
línea de comandos. Se utiliza para automatizar tareas repetitivas en el 
sistema de archivos Linux. Puede incluir un conjunto de comandos, o un solo 
comando, o puede contener las características de la programación imperativa como 
bucles, funciones, construcciones condicionales, etc. Efectivamente, un script 
Bash es un programa de computadora escrito en el lenguaje de programación Bash [11].

Bash, es un procesador de comandos que generalmente se ejecuta en una ventana de 
texto, donde el usuario escribe comandos que causan acciones. Bash también puede 
leer y ejecutar comandos desde un archivo, llamado script de shell.  Conocer los 
comandos y el manejo de scripts en Bash es esencial para llevar a cabo multitud de 
tareas bioinformáticas, el hecho de que nos permite realizar scripts que operan 
directamente sobre comandos del sistema, nos permitirá automatizar tareas, ahorrando 
mucho tiempo y esfuerzo [12].

\subsection*{Pipeless y Pipeline}

Un pipeless puede considerarse como un agente autónomo, experto en la 
realización de una tarea específica; recibe una o más entradas que pueden 
provenir del propio usuario o de otros pipeless [9].

Por otra parte, los pipelines constituyen una conexión secuencial de 
pipeless, y  la salida de un pipeless corresponderá a la entrada del siguiente. 
El pipeless de inicio puede contener varias entradas. La estrategia de pipelines 
expone una solución que permite desencadenar una serie de procesos controlados, 
realizando incluso pasos intermedios de formateo de datos según los requerimientos 
de cada pipeless en particular; esto optimiza tanto el uso de recursos informáticos 
como el tiempo, disminuyendo críticamente el tiempo muerto entre 
ejecución y ejecución [9].

Entre las experiencias de uso de pipelines se pueden mencionar trabajos 
relacionados con la comparación fenotípica, que incorpora la selección de propiedades 
y la distribución y estadística asociada a variables asociadas a la selección, además 
de la categorización y comparación final. Otro ejemplo de uso se asocia a la 
caracterización de funciones en un contexto evolutivo, donde se producen selecciones 
de secuencias, visualización y análisis de estructuras secundarias, identificación 
de interacciones sistémicas, análisis y visualización de resultados y por último la 
selección de datos [9].

En resumen, la estrategia de pipelines en distintos proyectos permite abordar de 
forma efectiva el sistema de gestión de datos, generando una nueva forma de abordaje 
a problemas complejos, especialmente en lo relacionado a la genómica comparativa. 
Cabe mencionar, que la creación de nuevos pipelines estará limitada sólo por la 
capacidad del usuario para generar flujos de trabajo adecuados que permitan su 
ejecución.

\subsection*{Flujo de trabajo con Nextflow}

Este texto, nos lleva a entender cómo los avances experimentados por las 
tecnologías de secuenciación han desembocado en un mayor número de experimentos 
ómicos (nuevos campos de investigación) que requieren de análisis reproducibles 
de cantidades ingentes de datos. En este sentido, las primeras aproximaciones 
de herramientas bioinformáticas -pipelines-, que se desarrollaron para este 
cometido, sufren de inestabilidad numérica (numerical instability) cuando 
el mismo pipeline se usa en diferentes plataformas, de forma que se imposibilita 
la reproducibilidad de los trabajos [14]. 

De acuerdo a lo anterior, Nextflow nace como lenguaje para la construcción de 
pipelines mejorando el uso de los recursos informáticos, controlando el 
flujo de datos de entrada y salida y manteniendo un registro de todas las 
actividades desarrolladas al fin de poder seguir el flujo de trabajo mismo, 
corregir errores eventuales y mantener un registro de los comandos ejecutados. 
Esta herramienta constituye un sistema de gestión de flujos de trabajo que utiliza 
distintas infraestructuras informáticas como Docker, Conda, Singularity, etc. 
para el manejo a gran escala mediante el uso de los que se conocen como “contenedores”. 
Estos últimos son entornos de computación configurados de tal forma que 
solamente admiten las entradas y salidas de datos para lo que se han 
desarrollado [14]. Las características que hacen de Nextflow una 
herramienta útil y flexible para la creación de pipelines son la posibilidad 
integración en repositorios de software como BitBucket, cuya página web se 
corresponde con la dirección https://bitbucket.org/ o GitHub, https://github.com/, 
un soporte nativo para sistemas en la nube. Además, se basa en el uso de 
contenedores multiescala, y de la paralelización de los procesos que se 
conectan mediante canales. Todas estas características mejoran la 
reproducibilidad computacional y posicionan a Nextflow como un lenguaje de 
dominio específico que adelanta a sus competidores. 

Un pipeline implementado en Nexflow puede contener cualquier 
lenguaje - incluso varios diferentes – compatible con Linux, como Python 
o Perl. Los procesos dentro de dicho pipeline estarían conectados mediante 
canales o colas FIFO asíncronas, ejecutándose en paralelo e independientemente 
cuando el input que requieren esté disponible. Un mismo proceso puede tener varios 
inputs y outputs, y diferentes procesos pueden tener un mismo input que llega a 
través de canales diferentes. En Nextflow se encuentran dos tipos distintos de 
canales: - Canal de colas (queue channel): cola FIFO no bloqueante 
y unidireccional. Este tipo de canal conecta dos procesos, es decir, el 
canal solo se puede usar una única vez como salida y solo una vez como 
entrada. En el caso de necesitar que se conecten varios procesos es posible 
duplicar – triplicar, cuadruplicar, … – un canal de colas mediante el 
operador into. - Canal de valores (value channel) o canal singleton: canal 
que contiene un solo valor, por lo que se puede emplear ilimitadamente 
como input sin ser consumido [14].

Por otro lado, Nextflow crea directorios únicos y 
temporales para cada proceso del pipeline, lo que implica que no es 
necesario organizar los outputs y facilita la gestión de estos, ya que no 
se requiere una estructura de directorios organizada o nombrar los archivos 
de forma estática para su identificación inequívoca. Más aún, en cada proceso 
no es imprescindible nombrar cada archivo de salida, pues al existir directorios 
únicos para cada proceso, no se produce la superposición entre archivos. También, 
es posible relacionar los outputs con metadatos mediante el operador tuple para 
evitar incluirlos en el nombre del archivo de salida [14].

\subsection*{Contenerización}

El procedimiento de contenerización se ha convertido en la última palabra en 
cuanto a informática en la nube,  Es más eficiente que la virtualización, y es 
su evolución natural. Mientras que la virtualización es vital para distribuir 
varios sistemas operativos (SO) en un único servidor, la contenerización es más 
flexible y granular. Se centra en dividir los sistemas operativos en fragmentos 
que pueden utilizarse de manera más eficiente. Además, un contenedor de aplicaciones 
proporciona una forma de empaquetar aplicaciones en un entorno portátil definido por 
software [15]. 

Es una forma de virtualización del SO en la que se ejecutan aplicaciones en 
espacios de usuario aislados llamados contenedores, que utilizan el mismo SO 
compartido. Un contenedor de aplicaciones es un entorno informático portátil y 
totalmente empaquetado:

\begin{itemize}
    \item Tiene todo lo que una aplicación necesita ejecutar, incluidos sus 
        binarios, bibliotecas, dependencias y archivos de configuración, todo 
        encapsulado y aislado en un contenedor.
    \item La contenerización de una aplicación abstrae el contenedor del 
        sistema operativo host, y proporciona acceso limitado a los recursos 
        subyacentes, un proceso similar al de una máquina virtual ligera.
    \item Puede ejecutar la aplicación en contenedores en varios tipos de 
        infraestructuras, como en estructuras físicas, en la nube o dentro de 
        máquinas virtuales, sin tener que refactorizar para cada entorno [15].
\end{itemize}

Aunque los conceptos de contenerización y aislamiento de procesos tienen 
décadas de antigüedad, la aparición de un motor de Docker de código abierto en 
2013 aceleró la adopción de la tecnología de contenedores de aplicaciones. 
El motor de Docker se convirtió en un estándar de la industria para el proceso 
de contenerización con un enfoque de empaquetado universal y herramientas de 
desarrollo simples [15].

\subsection*{Motor de Docker}

Docker es una plataforma de código abierto que permite a los desarrolladores 
crear, desplegar, ejecutar y gestionar contenedores, que son componentes 
estandarizados y ejecutables que combinan el código fuente de aplicación con 
las dependencias y las bibliotecas del sistema operativo (SO) necesarias para 
ejecutar dicho código en cualquier entorno [16].

Los contenedores simplifican el desarrollo y la entrega de aplicaciones 
distribuidas y han ido ganando popularidad con la transición de las organizaciones 
hacia entornos híbridos multi nube y de desarrollo nativo en la nube.  
Los desarrolladores pueden crear contenedores sin Docker trabajando directamente 
con las funciones integradas en Linux y otros sistemas operativos, pero la 
contenerización que realiza Docker es más rápida, más fácil y más segura [16].

La tecnología Docker utiliza el kernel de Linux y sus funciones, como los grupos 
de control y los espacios de nombre, para dividir los procesos y ejecutarlos de 
manera independiente. El propósito de los contenedores es ejecutar varios procesos 
y aplicaciones por separado para que se pueda aprovechar mejor la infraestructura y, 
al mismo tiempo, conservar la seguridad que se obtendría con los 
sistemas individuales. [17] Las herramientas de los contenedores, 
como Docker, proporcionan un modelo de implementación basado en imágenes. 
Esto permite compartir fácilmente una aplicación o un conjunto de servicios, 
con todas las dependencias en varios entornos. Docker también automatiza la 
implementación de las aplicaciones (o los conjuntos de procesos que las constituyen) 
en el entorno de contenedores [17].

Estas herramientas están diseñadas a partir de los contenedores de Linux, por 
eso la tecnología Docker es sencilla y única. Además, ofrecen a los usuarios 
acceso sin precedentes a las aplicaciones, la posibilidad de realizar 
implementaciones en poco tiempo y el control sobre las versiones y 
su distribución [17].

\newpage
\section{Plantamiento del Problema}
El rápido avance  de la secuenciación de genomas bacterianos, ha llevado a una acumulación masiva de datos genómicos que requieren análisis profundos que permitan comprender la diversidad, evolución y funcionalidad de estos microorganismos. Sin embargo, el acceso a herramientas de análisis adecuadas y la interpretación de los resultados siguen siendo un desafío para muchos científicos del área, especialmente en lo que respecta a la reproducibilidad y escalabilidad de los flujos de trabajo bioinformáticos. Se hace imprescindible entonces, disponer de un flujo predefinido, que permita automatizar el proceso, haciendo que cada etapa del trabajo sea visible, facilitando de esta manera, el control sobre el avance de la tarea. sin necesidad de un aumento de los recursos,  disminuyendo la posibilidad de errores y  propiciando una evolución constante que permita mayor y mejor accesibilidad a otros investigadores.

\newpage
\section{Objetivos}
\subsection{Objetivo general}
Desarrollar un Pipeline Bioinformático que integre herramientas  computacionales necesarias para el procesamiento y  análisis genómico de bacterias noveles con potencial biotecnológico, permitiendo de esta manera la generación de reportes automatizados.
\subsection{Objetivos específicos}
\begin{enumerate}
    \item Priorizar los métodos de análisis genómico y anotación funcional principales para la exploración de bacterias noveles con potencial biotecnológico.
    \item Desarrollar un pipeline bioinformático basado en la caracterización de necesidades de análisis genómicos previos.
    \item Definir estructura del reporte Automatizar la generación de reportes de resultados de secuenciación y de anotación funcional a partir de datos genómicos.
\end{enumerate}


\newpage
\section{Estado del arte}
En este  capítulo se plantea una revisión del estado del arte en relación a los flujos bioinformáticos existentes, que permiten mayor eficiencia en el análisis de la genómica bacteriana.
De acuerdo a lo anterior, se presentan algunas experiencias vinculadas a flujos de trabajo, con respecto  a la obtención de información metagenómica.
Este capítulo permitirá visualizar enfoques metagenómicos aplicados a distintas investigaciones, flujos bioinformaticos que permiten la aplicación de pipelines asociados a  la información que se obtiene de las etapas de cada investigación.

\subsection*{Metagenomic and network analysis revealed wide distribution of antibiotic resistance genes in monkey gut microbiota}
Este artículo tiene como objetivo utilizar el enfoque metagenómico, para descubrir la comunidad bacteriana en el mono NHP - cynomolgus. Se busca identificar diferentes genes de resistencia antibiótica, explorar la relación entre las bacterias, para identificar posibles riesgos para la salud, a partir de las heces del mono.

Los autores mencionan, el flujo de trabajo realizado posterior a la recopilación de datos, para realizar un análisis bioinformático. Se realizó en primer lugar un análisis , a través de la herramienta MetaPhlAn2, y una visualización del árbol taxonómico utilizando GrapPhlAn. Además, se explica detalladamente cómo se generó un modelo de aprendizaje automático utilizando el algoritmo RF (Bosque Aleatorio) en Python.  En el caso de la anotación funcional de genes de resistencia antibiótica, se explica que para realizar la anotación de todas las proteínas identificadas, se realizó una búsqueda de similitud de secuencia con la herramienta Diamond Blastp, usando la base de datos ARGminer. Los últimos análisis corresponden a análisis estadísticos y de redes usando la plataforma R versión 3.5.1. De esta manera, se evaluó las diferencias en la abundancia de genes de resistencia antibiótica entre diferentes grupos de tratamiento dietético, se realizaron visualizaciones de las relaciones entre las muestras de cada grupo de monos cynomolgus y una visualización aluvial para comprender cómo los genes se distribuyen.

Finalmente, con el objetivo de revelar cuáles son las diferencias entre los genes de NHP y de humanos como una red, para obtener más información sobre la obesidad, diabetes tipo II y diferentes enfermedades coronarias en el humano debido a la similitud con el ser humano.



\subsection*{Overview of bioinformatic methods for analysis of antibiotic resistome from genome and metagenome data}
Este artículo menciona los diferentes flujos de trabajo bioinformáticos que se realizan para el análisis de resistencia antibiótica ya sea a partir de un genoma o de información metagenómica.  De esta manera, se expone la importancia que tiene el secuenciar genomas completos, y el que tan necesario es poder analizar los datos, recurriendo a flujos de trabajo bioinformáticos. Cada flujo de trabajo, posee diferentes herramientas y recursos específicos que ayudan al análisis. Dentro de los flujos de trabajo, el primero describe el pre procesamiento y ensamblaje de las lecturas de datos de WGS destacando la importancia que tiene la calidad de las lecturas. Dentro de este flujo se habla sobre omitir el paso del ensamble con el fin de acelerar el proceso computacional en bacterias monomórficas. En otros casos, van desde la identificación taxonómica a nivel de especie, nivel de mutación y de cepa.

\subsection*{LGAAP: Leishmaniinae Genome Assembly and Annotation Pipeline}
Este artículo describe un proceso computacional para el ensamblaje y anotación de genomas de un tamaño aproximado de ~35 mb. El autor, menciona qué parámetros utilizan, en qué orden fueron ejecutados las herramientas y qué datos fueron analizados.  De esta manera, se utilizaron 6 genomas de la subfamilia del parásito Leishmaniinae, que presentaba lecturas cortas, y largas. Además se describe cuales son los archivos de salida, que son el ensamblaje a escala de cromosomas, las proteínas y transcripciones en formato FASTA, dos archivos GFF, uno con las características y otro con las coordenadas.

\subsection*{A Pipeline for Non-model Organisms for de novo Transcriptome Assembly, Annotation, and Gene Ontology Analysis Using Open Tools: Case Study with Scots Pine }
Esta publicación describe el flujo bioinformático utilizado para el ensamblaje del transcriptoma, la anotación y el análisis de ontología genética del pino silvestre Pinus Sylvestris. El procedimiento descrito muestra un requisito básico de conocimientos en bioinformática  y línea de comandos linux. De esta manera, el autor realiza un análisis que describe y clasifica los genes y productos génicos de acuerdo a su función molecular, implicación en los procesos biológicos y localización celular.


\newpage
\section{Materiales y métodos}

\newpage
\section{Resultados}

\newpage
\section{Discusión}
La búsqueda bibliográfica permitió identificar diferentes herramientas que permiten una correcta ejecución del flujo de trabajo del grupo seleccionado para la identificación de genes de resistencia antibiótica, se tuvo dificultades para instalar y ejecutar la gran mayoría de las opciones. La documentación, reflejaba una instalación basada en contenedor, lo que dificulta la instalación del ejecutable en nuestro propio ambiente de ejecución. Es por lo anterior, que se prefirió la utilización de un software que utiliza las bases de datos para la comparación como Resfinder o de NCBI para generar la identificación y anotación de los genes. 

El diseño del flujo de trabajo, no presenta la posibilidad de seleccionar la herramienta a usar para los análisis, si no que por el contrario, existen herramientas predefinidas, detrás de la activación de la funcionalidad del usuario. Aquí, nace la posibilidad  de evaluar si en futuras versiones, darle la cualidad al usuario de seleccionar la herramienta a usar por análisis mejoraría la usabilidad y rendimiento del flujo de trabajo. 

El diseño actual, presente en este trabajo brinda la posibilidad de ejecutar diferentes análisis, con el único objetivo de mejorar la automatización de diversas herramientas bioinformáticas, sin duda, el servicio de este flujo de trabajo puede ser un instrumento potencialmente favorable para su utilización en investigaciones. De lo anterior, nace la problemática de seguir mejorando la automatización, optimizando el código, encontrando nuevas funcionalidad que vayan escalando junto al lenguaje de Nextflow.

Dentro del flujo de trabajo se optó por agregar diferentes funcionalidades que le ofrezcan al usuario una mejor “calidad de vida” y una mayor automatización, como la descarga de genomas de referencia, la utilización de SRA Toolkit para la descarga de las muestras en formato .fastq, la posibilidad de descomprimir directamente los archivos desde el flujo de trabajo o como también la descarga de las bases de datos para el análisis taxonómico. Sin duda, ninguna de estas funcionalidades intenta aumentar el tamaño de la imágen de docker, o disminuir el rendimiento, se tendrá que evaluar la usabilidad de cada una de estas características.

Una de las funcionalidades más importantes, es la implementación de un reporte final automatizado, que brinda la posibilidad al usuario de obtener la información de los procesos ejecutados de manera resumida. Sin duda, la implementación marcó un desafío, ya que, el lenguaje base del flujo de trabajo está escrito en Nextflow. Este lenguaje no brinda la funcionalidad de ejecutar otro proceso terminado el flujo de trabajo principal, que serían todos los procesos seleccionados por el usuario. De esta manera, se tuvo que generar un canal para generar una dependencia y que reciba los resultados de cada proceso ejecutado y así detectar el momento en que los procesos del “flujo principal” terminen. Esta manera de detectar cuándo ejecutar el proceso que realiza el informe, sin duda, marca una lentitud en el rendimiento general y puede ser mejorada con alguna actualización de Nextflow futura.

	
La ejecución de los casos de estudios, se realizaron bajo un ambiente de bajo rendimiento, en un computador con 7 GB de RAM lo cual limitó nuestra capacidad de procesamiento de datos. Las restricciones de memoria afectaron significativamente la capacidad para ejecutar herramientas bioinformáticas intensivas en recursos como Kraken 2 y Spades de manera eficiente. Para manejar estas limitaciones, utilizamos varias estrategias. Ocupamos versiones más ligeras de las bases de datos y ejecutamos los análisis en fragmentos más pequeños de datos en lugar de analizar muestras completas de una vez. Lo cual, se puede observar, en la base de datos utilizada en el proceso de identificación taxonómica donde se utilizó la colección de muestras virales de un tamaño de 0.5 GB . Otro ejemplo claro, es la utilización de las primeras 500 lecturas de escherichia coli y no el genoma de la muestra completo. Además, se implementaron técnicas de optimización de memoria, como la liberación de caché y el uso de datos comprimidos que permitieron completar los análisis, pero también presentaron desafíos, como tiempos de ejecución prolongados y la necesidad de realizar múltiples repeticiones para verificar los resultados. Es por esto, que para futuros usos, se recomienda el uso de sistemas con mayor capacidad de memoria acelerando los tiempos de análisis,y brindando la opción de usar datos de entrada más grandes. Es decir, las limitaciones de hardware presentaron desafíos significativos, y que si no es por las estrategias o adaptaciones, no se hubieran llevado a cabo. Sin embargo, mejorar los recursos computacionales es esencial para optimizar futuros estudios en análisis bioinformáticos y genómica bacteriana.

La documentación del flujo de trabajo en Github, está estructurada de forma que tiene secciones bien definidas que guían al usuario desde la instalación inicial, la configuración hasta la ejecución completa del pipeline. Cada sección incluye una descripción que ayuda a los usuarios a comprender no sólo el cómo funcionan si no que también el por qué detrás de cada etapa. Sin embargo, es claro mencionar que algunas secciones podrían mejorar su claridad con ejemplos más elaborados, y explicaciones adicionales.
	
Es importante que la documentación se mantenga actualizada con cualquier cambio del flujo de trabajo, o del repositorio. Así mismo, al usar Github, existe la posibilidad de abrir issues y discusiones en el repositorio lo que permite que los usuarios puedan colaborar tanto en la mejora de la documentación como en el flujo.

El empaquetamiento del flujo de trabajo, permite poder garantizar la reproducibilidad sin importar el ambiente de ejecución del usuario, sin embargo, es necesario seguir mejorando y optimizando la imagen final a utilizar. En el caso de que en el trabajo futuro se agreguen más herramientas y dependencias, se hace imprescindible la necesidad de tener una imagen base más liviana. El flujo de trabajo actual, utiliza la última versión de ubuntu hasta la fecha, esto permite tener más facilidad al tener una gama de dependencias base más amplia pero con la consecuencia de un mayor peso neto en la descarga. 



\newpage
\section{Conclusión}
Los flujos de trabajo automatizados, reducen el tiempo necesario para realizar análisis y permite al investigador enfocarse, concentrarse en la interpretación de los resultados. De esta manera, optimizando el uso de recursos computacionales y el capital humano. En investigaciones que requieren análisis rápidos, la utilización de flujos de trabajo permite obtener una visualización general del panorama a investigar. Sin embargo, es importante destacar la capacidad dinámica y versátil de las herramientas bioinformáticas, las cuales ofrecen una gran flexibilidad para adaptarse a diferentes necesidades y contextos de investigación. Ofreciendo una mayor capacidad, cuando se tiene un mayor conocimiento sobre la herramienta.


El diseño del flujo de trabajo actual, permite poder escalar fácilmente para manejar un mayor conjunto de herramientas o para manejar grandes volúmenes de datos. Lo anterior, es especialmente importante en estudios de área de la genómica bacteriana, donde se pueden realizar múltiples muestras simultáneamente. Es decir, la capacidad de integrar diversas herramientas bioinformáticas dentro de un único flujo de trabajo permite un análisis más completo y detallado donde puede adaptarse y personalizarse para satisfacer necesidades específicas de diferentes contextos. 


La documentación detallada, que abarca desde la instalación hasta ejemplos de uso, facilita la utilización de flujos de trabajo por parte de investigadores con diferentes niveles de experiencia en bioinformática. Esto permite a la comunidad contribuir a la mejora continua de estos flujos de trabajo mediante la actualización de herramientas y la incorporación de nuevas metodologías a través de solicitudes de cambio en GitHub. De esta manera, se asegura que los flujos de trabajo se mantengan al día con los avances tecnológicos, así como la corrección de errores y la implementación de mejoras en la calidad de vida.


\newpage
El cálculo de la composición relativa de TEs de cada gen será obtenido de la siguiente forma:
\begin{equation}
    \frac{N_{TE}}{N_{Total}} = F_{TE}
    \label{eq:freqRelativa}
\end{equation}
Donde $N_{TE}$ es la cantidad de observaciones de una familia o superfamilia de TEs particular y $N_{Total}$ es la cantidad total de TEs observados en cada gen y $F_{TE}$ es la frecuencia relativa de una familia o superfamilia respecto al total de TEs observados. Con esto obtuvimos los vectores T20R, N75R y T6R, donde la R al final denota que es un vector de frecuencia relativa.

\begin{figure}[ht!]
    \centering
    \small
    \vspace*{-10mm}
    \includegraphics[scale=0.6]{T6N-M-In.png}
    \caption{Comparación entre los promedios de distancias euclidianas promedio entre rutas metabólicas (o distancias internas promedio) usando como vector T6R. Las líneas verticales de color rojo y azul representan el promedio y la mediana, respectivamente. Los promedios y medianas se encuentran en su contexto de datos en forma de histograma (barras verticales) y la correspondiente curva de densidad de kernel estimada (o KDE, representada por la línea continua en azul, que aproximadamente contorna el histograma). a) (Arriba) Rutas metabólicas HKG (promedio = 0,1598; mediana = 0,0669; n = 10); b) (Abajo) Rutas metabólicas no-HKG (promedio = 0,3031; mediana = 0,3198; n = 235). El eje X está en la misma escala para comparación. El eje Y no está en la misma escala para mejor visualización.}
    \label{T6R-M-In}
\end{figure}

\begin{table}[ht!]
    \centering
    \begin{tabular}{|c|c|c|c|} % Cantidad de celdas y como alinear el texto
        \hline % Línea horizontal entre filas 
        \textbf{Tipo de vector} & \textbf{HKG} & \textbf{No-HKG} & \textbf{Parcial-HKG} \\ \hline 
        T6N & \makecell{\textbf{N}: 26 $\boldsymbol\mu$: 0,1154 \\ \textbf{M}: 0,1167 \textbf{\emph{s}}: 0,0800 } &
        \makecell{\textbf{N}: 3.457 $\boldsymbol\mu$: 0,1373 \\ \textbf{M}: 0,1332 \textbf{\emph{s}}: 0,0744 } &
        \makecell{\textbf{N}: 22.155 $\boldsymbol\mu$: 0,1447 \\ \textbf{M}: 0,1409 \textbf{\emph{s}}: 0,0782 } \\ \hline
        T6R & \makecell{\textbf{N}: 26 $\boldsymbol\mu$: 0,2111 \\ \textbf{M}: 0,1848 \textbf{\emph{s}}: 0,1697 } &
        \makecell{\textbf{N}: 3.457 $\boldsymbol\mu$: 0,3467 \\ \textbf{M}: 0,2962 \textbf{\emph{s}}: 0,2195 } &
        \makecell{\textbf{N}: 22.155 $\boldsymbol\mu$: 0,3698 \\ \textbf{M}: 0,3178 \textbf{\emph{s}}: 0,2247 }\\ \hline
        T6-OCC & \makecell{\textbf{N}: 26 $\boldsymbol\mu$: 0,2323 \\ \textbf{M}: 0,2166 \textbf{\emph{s}}: 0,1719 } &
        \makecell{\textbf{N}: 3.457 $\boldsymbol\mu$: 0,4432 \\ \textbf{M}: 0,4067 \textbf{\emph{s}}: 0,2518 } &
        \makecell{\textbf{N}: 22.155 $\boldsymbol\mu$: 0,4411 \\ \textbf{M}: 0,4069 \textbf{\emph{s}}: 0,2444 } \\ \hline

    \end{tabular}
    \caption{Resumen de distancias entre parejas de genes pertenecientes a una misma ruta (distancia interna). Las filas representan el tipo de vector que se esta usando en las rutas de cada columna . \textbf{N} es la cantidad comparaciones de parejas de genes. $\boldsymbol\mu$ es la media de distancias. \textbf{M} es la mediana de las distancias y \textbf{\emph{s}} es la desviación estándar.}
    \label{res1}
\end{table}
\clearpage 
\section{Anexos}
\subsection{Anexo 1: Cálculo de vectores de composición}

\begin{figure}[ht!]
    \centering
    \small
    \includegraphics[scale=0.35]{barplot.png}
    \caption{Comparación entre las proporciones del número de TEs anotados en zonas intrónicas (color cyan) vs. zonas intergénicas (color salmón). Cada barra representa un organismo modelo. De izquierda a derecha: \emph{Drosophila melanogaster} (mosca de la fruta); \emph{Danio rerio} (Pez zebra); \emph{Homo sapiens} (humano) y, \emph{Mus musculus} (ratón).}
    \label{fig:distribucion}
\end{figure}

Para ilustrar mejor cómo se crean estos vectores tomemos en consideración el siguiente ejemplo.
Un determinado gen contiene los siguientes TEs en sus intrones (ejemplo real de un archivo de conteos):
\begin{verbatim}
     34 Alu
      2 CR1
      2 ERVL
      1 ERVL-MaLR
      1 hAT-Blackjack
      6 hAT-Charlie
     19 L1
      9 L2
     18 MIR
      3 RTE-X
      1 TcMar-Tigger
\end{verbatim}
Al ser este convertido a un vector de conteo absoluto obtenemos los siguientes conteos: 
\begin{equation*}
    Vec_{T20} =
    \begin{Bmatrix}
        Alu\\
        L1\\
        MIR\\
        L2\\
        ERVL-MaLR\\
        hAT-Charlie\\
        ERV1\\
        ERVL\\
        TcMar-Tigger\\
        CR1\\
        hAT-Tip100\\
        hAT-Blackjack\\
        Gypsy\\
        TcMar-Mariner\\
        RTE-X\\
        ERVK\\
        RTE-BovB\\
        hAT\\
        TcMar-Tc2\\
        Others\\
    \end{Bmatrix}
    =
    \begin{Bmatrix}
34\\
19\\
18\\
9\\
1\\
6\\
0\\
2\\
1\\
2\\
0\\
1\\
0\\
0\\
3\\
0\\
0\\
0\\
0\\
0\\
    \end{Bmatrix}
\end{equation*}
Dado que este gen tiene unbenaj total de 96 TEs, el vector de composición relativa se obtiene dividiendo cada una de las dimensiones del vector por este número total, obteniendo el siguiente vector: 

\begin{equation*}
    Vec_{T20R} =
    \begin{Bmatrix}
        Alu\\
        L1\\
        MIR\\
        L2\\
        ERVL-MaLR\\
        hAT-Charlie\\
        ERV1\\
        ERVL\\
        TcMar-Tigger\\
        CR1\\
        hAT-Tip100\\
        hAT-Blackjack\\
        Gypsy\\
        TcMar-Mariner\\
        RTE-X\\
        ERVK\\
        RTE-BovB\\
        hAT\\
        TcMar-Tc2\\
        Others\\
    \end{Bmatrix}
    =
    \begin{Bmatrix}
	0.3541666666666667\\
	0.19791666666666666\\
	0.1875\\
	0.09375\\
	0.010416666666666666\\
	0.0625\\
	0.0\\
	0.020833333333333332\\
	0.010416666666666666\\
	0.020833333333333332\\
	0\\
	0.010416666666666666\\
	0.0\\
	0.0\\
	0.03125\\
	0.0\\
	0.0\\
	0.0\\
	0.0\\
	0.0\\
    \end{Bmatrix}
\end{equation*}
Podemos aplicar esta misma lógica para construir los vectores de ocupancia, solo debemos reemplazar los conteos por la ocupancia de una familia de TEs al interior del gen, y dividirlo por la suma total.
A continuación se detalla la forma que tienen los 2 tipos de vectores restantes. Los vectores N75 tienen la siguiente forma:

\clearpage
\singlespacing % reduce el espacio entre citas
\bibliographystyle{ieeetr} % Estilo de citación, cambia como se escribe al final y como aparecen las citas en texto
\bibliography{main.bbl} % Las referencias están en un archivo aparte

\end{document}
